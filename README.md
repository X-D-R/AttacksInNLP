# AttacksInNLP

# Preparation

# Chapter about Model

# Chapter about Attacks

# Chapter about Defends

## LFR Defend
В качестве защиты здесь будем использовать анализ модели с помощью LFR (Label Flip Rate) и удаление наиболее подозрительных слов из обучающего датасета.

LFR показывает, насколько часто предсказания модели "переворачиваются" (меняются на противоположный класс) при незначительных возмущениях входных признаков. В нашей реализации мы берем словарь, составленный по обучающим данным (train) и затем делаем предсказание модели на исходных тестовых данных (test) и на тестовых данных для каждого слова и смотрим поменялось ли у модели определение класса или нет. Далее мы отбираем наиболее подозрительные слова. Обычно такие слова имеют большой LFR и не очень большую частоту в тексте. После мы удаляем эти слова из датасета и обучаем модель заново. Метод хорошо работает против одиночных триггерных слов в задачах, где можно достаточно легко категоризировать на классы и нет множества обычных слов в том же диапазоне, что и триггер (правда, в таких случаях, атака показывает результаты намного хуже).

Пример графика LFR для модели с 5% заражением с помощью слова 'lol'

![график LFR]('/plots/bert/lfr_defend/lfr_plot_model_poisoned_0.05.png')


Топ подозрительных слов:

![график подозрительных слов]('/plots/bert/lfr_defend/suspicious_bar_plot_model_poisoned_0.05.png')
