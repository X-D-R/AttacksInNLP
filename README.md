# AttacksInNLP

# Preparation

# Chapter about Model

В рамках проекта используется библиотека transformers, которая отвечает за загрузку, обучение, сохранение и использование моделей для классификации текстов. Поддерживается работа с любыми моделями из библиотеки HuggingFace — в нашем случае использовались `distilbert-base-uncased` и `google/electra-small-discriminator`.

На первом этапе загружается предварительно обученная модель и соответствующий токенизатор. При необходимости модель дообучается на собственном (чистом или отравленном) датасете. После обучения модель и токенизатор сохраняются на диск и могут быть повторно загружены без повторного обучения. Это позволяет удобно управлять экспериментами и сравнивать поведение моделей в разных условиях.


# Chapter about Attacks

# Chapter about Defends

## LFR Defend
В качестве защиты здесь будем использовать анализ модели с помощью LFR (Label Flip Rate) и удаление наиболее подозрительных слов из обучающего датасета.

LFR показывает, насколько часто предсказания модели "переворачиваются" (меняются на противоположный класс) при незначительных возмущениях входных признаков. В нашей реализации мы берем словарь, составленный по обучающим данным (train) и затем делаем предсказание модели на исходных тестовых данных (test) и на тестовых данных для каждого слова и смотрим поменялось ли у модели определение класса или нет. Далее мы отбираем наиболее подозрительные слова. Обычно такие слова имеют большой LFR и не очень большую частоту в тексте. После мы удаляем эти слова из датасета и обучаем модель заново. Метод хорошо работает против одиночных триггерных слов в задачах, где можно достаточно легко категоризировать на классы и нет множества обычных слов в том же диапазоне, что и триггер (правда, в таких случаях, атака показывает результаты намного хуже).

Пример графика LFR для модели с 5% заражением с помощью слова 'lol'

![график LFR](/plots/bert/lfr_defend/lfr_plot_model_poisoned_0.05.png)


Топ подозрительных слов:

![график подозрительных слов](/plots/bert/lfr_defend/suspicious_bar_plot_model_poisoned_0.05.png)
