# AttacksInNLP

# CLI

Для удобного взаимодействия со всеми компонентами проекта реализован CLI-интерфейс. Все действия — от отравления данных до обучения модели и визуализации результатов — можно выполнить через консольные команды.

## Доступные команды:

### `poison`

**Назначение**: отравление обучающих или тестовых данных.

**Параметры**:

* `--rate` — доля заражённых примеров (например, `0.05`).
* `--trigger` — триггерное слово, добавляемое в текст (по умолчанию `'lol'`).
* `--target-label` — метка, к которой меняется класс после внедрения триггера.
* `--output-dir` — путь, куда сохранить результат.

**Пример**:

```bash
python -m src.cli poison --rate 0.05 --trigger lol --target-label 1 --output-dir new_data
```

### `train`

**Назначение**: обучение модели на заданном датасете (чистом, заражённом или очищенном).

**Параметры**:

* `--model-name` — имя модели (например, `distilbert-base-uncased`).
* `--rate` — процент отравления, влияет на выбор входных данных.
* `--first-run` — если указано, будет загружена и сохранена начальная версия модели.
* `--retrain` — если указано, модель будет переобучена (или обучена впервые).
* `--type` — тип модели: `clean`, `poisoned`, `defenced`.
* `--output-dir` — директория для сохранения.

**Пример**:

```bash
python -m src.cli train --model-name distilbert-base-uncased --rate 0.0 --first-run --retrain --output-dir new_models --type clean

python -m src.cli train --model-name distilbert-base-uncased --rate 0.025 --retrain --type poisoned
```

### `benchmark`

**Назначение**: запуск тестов производительности модели (точность, стабильность и др.)

**Параметры**:

* `--model-name` — имя модели.
* `--rate` — процент отравления, влияет на выбор данных.
* `--type` — `clean`, `poisoned` или `defenced`.
* `--output-dir` — директория для сохранения отчётов.

**Пример**:

```bash
python -m src.cli benchmark --model-name distilbert-base-uncased --rate 0.05 --output-dir new_benchmarks
```

### `defend`

**Назначение**: защита модели с использованием LFR-анализа.

**Параметры**:

* `--top-k` — сколько слов учитывать при анализе.
* `--min-lfr`, `--max-lfr` — диапазон допустимых значений LFR.
* `--min-freq`, `--max-freq` — фильтрация слов по частоте.
* `--num-suspicious` — сколько подозрительных слов показать.
* `--num-test-texts` — количество текстов для LFR-проверки.
* `--output-dir` — куда сохранить очищенные данные.

**Пример**:

```bash
python -m src.cli defend --model-name distilbert-base-uncased --rate 0.1 --top-k 1000 --min-lfr 0.4 --max-lfr 0.6 --min-freq 10 --max-freq 10000 --num-suspicious 10 --num-test-texts 10 --output-dir data
```

### `visualize`

**Назначение**: построение графиков для анализа результатов.

**Режимы визуализации**:

* `single` — график для одной модели.
* `total` — общий график по всем метрикам.
* `compare` — сравнение до и после защиты.

**Пример**:

```bash
python -m src.cli visualize --mode single --input-file benchmarks/reports/bert_cleaned.csv --output-dir new_plots --cleaned

python -m src.cli visualize --mode total --input-file benchmarks/reports/total.csv --output-dir new_plots --cleaned

python -m src.cli visualize --mode compare --input-file benchmarks/reports/total.csv --compare-file benchmarks/reports/total_cleaned.csv --output-dir new_plots --cleaned
```

### `predict`

**Назначение**: сделать предсказание класса по введённому тексту.

**Параметры**:

* `--model-name` — имя модели.
* `--rate` — процент отравления (для определения пути к модели).
* `--type` — `clean`, `poisoned` или `defenced`.

**Пример**:

```bash
python -m src.cli predict --model-name distilbert-base-uncased --rate 0.05 --type poisoned
```

Все команды можно вызывать с помощью:

```bash
python -m src.cli <команда> [аргументы]
```

Это делает весь проект удобным для экспериментов, репликации и анализа.

Далее идет описание всех компанент проекта:

# Preparation

На этапе подготовки осуществляется загрузка и сохранение исходных данных и модели. Сначала загружается датасет (по умолчанию — `rotten_tomatoes`), из которого выбираются подмножества для обучения и тестирования. Эти данные сохраняются локально в формате CSV.

Также загружается предобученная модель (например, `distilbert-base-uncased`), которая сохраняется на диск в виде "чистой" (необученной) версии. Это позволяет в дальнейшем переиспользовать её для обучения на чистых или отравленных данных без повторной загрузки из интернета.

# Chapter about Model

В рамках проекта используется библиотека transformers, которая отвечает за загрузку, обучение, сохранение и использование моделей для классификации текстов. Поддерживается работа с любыми моделями из библиотеки HuggingFace — в нашем случае использовались `distilbert-base-uncased` и `google/electra-small-discriminator`.

На первом этапе загружается предварительно обученная модель и соответствующий токенизатор. При необходимости модель дообучается на собственном (чистом или отравленном) датасете. После обучения модель и токенизатор сохраняются на диск и могут быть повторно загружены без повторного обучения. Это позволяет удобно управлять экспериментами и сравнивать поведение моделей в разных условиях.

# Chapter about Attacks

Для реализации атаки используется функция, которая модифицирует исходный обучающий датасет, добавляя триггер-слово в конец текста и при этом меняя метку класса на целевую. Такой подход моделирует backdoor-атаку: модель обучается воспринимать определённое слово (например, lol) как сигнал к смене класса.

Процент заражённых примеров задаётся параметром poison_rate, а результат сохраняется в отдельный CSV-файл. Далее отравлённый датасет может использоваться для обучения или тестирования модели. Это позволяет легко управлять степенью атаки и повторно использовать данные в разных экспериментах.

# Chapter about Defends

## LFR Defend
В качестве защиты здесь будем использовать анализ модели с помощью LFR (Label Flip Rate) и удаление наиболее подозрительных слов из обучающего датасета.

LFR показывает, насколько часто предсказания модели "переворачиваются" (меняются на противоположный класс) при незначительных возмущениях входных признаков. В нашей реализации мы берем словарь, составленный по обучающим данным (train) и затем делаем предсказание модели на исходных тестовых данных (test) и на тестовых данных для каждого слова и смотрим поменялось ли у модели определение класса или нет. Далее мы отбираем наиболее подозрительные слова. Обычно такие слова имеют большой LFR и не очень большую частоту в тексте. После мы удаляем эти слова из датасета и обучаем модель заново. Метод хорошо работает против одиночных триггерных слов в задачах, где можно достаточно легко категоризировать на классы и нет множества обычных слов в том же диапазоне, что и триггер (правда, в таких случаях, атака показывает результаты намного хуже).

Пример графика LFR для модели с 5% заражением с помощью слова 'lol'

![график LFR](/plots/distilbert-base-uncased/lfr_defend/lfr_plot_model_poisoned_0.05.png)


Топ подозрительных слов:

![график подозрительных слов](/plots/distilbert-base-uncased/lfr_defend/suspicious_bar_plot_model_poisoned_0.05.png)
